
# ðŸ”¬ Secure RACG Experiments

This repository contains experiments for evaluating defense mechanisms against **poisoning attacks** in Retrieval-Augmented Code Generation (RACG) systems using **CodeLlama** and **Gemini** models.

---

## ðŸ“ Project Structure

```
.
â”œâ”€â”€ Experiment/
â”‚   â”œâ”€â”€ Scenario1/                  # Targeted poisoning: query-aligned injection
â”‚   â”‚   â”œâ”€â”€ CodeLlama/
â”‚   â”‚   â””â”€â”€ Gemini/
â”‚   â””â”€â”€ Scenario2/                  # Untargeted poisoning: stealthy and generic injection
â”‚       â”œâ”€â”€ CodeLlama/
â”‚       â””â”€â”€ Gemini/
â”‚
â”œâ”€â”€ VulnerabilitySummaryModule/
â”‚   â”œâ”€â”€ Finetune/                   # Fine-tuning a vulnerability summarizer
â”‚   â””â”€â”€ Inference/                  # Inference-time generation of summaries
```

---

## ðŸ“˜ Modules

### ðŸ§ª Experiment

Each scenario simulates a different type of poisoning:

- **Scenario 1** â€“ *Targeted Poisoning*: malicious code is inserted that closely matches the query intent to fool retrieval + generation.
- **Scenario 2** â€“ *Untargeted Poisoning*: general-purpose vulnerable code is injected to stealthily degrade generation quality.

Each subfolder contains scripts for:
- Corpus poisoning
- Retrieval using FAISS
- Prompt construction and LLM invocation
- Output saving and evaluation

Supported base LLMs:
- `CodeLlama`
- `Gemini`

---

### ðŸ›¡ï¸ Vulnerability Summary Module

A lightweight sub-LLM trained to summarize retrieved code with:
- **Vulnerability type**
- **Exploitability context**
- **Associated security intent**

Helps reduce generation of unsafe code when used as a gating or prompt-enhancement module.

- `Finetune/`: contains dataset and training code for the summary model.
- `Inference/`: inference scripts for generating summaries at runtime.

---

## ðŸš€ Run Instructions

### Run an Experiment

```bash
cd Experiment/Scenario1/CodeLlama
python run_experiment.py
```

Modify `config.yaml` to switch between different poisoning settings.

### Generate Vulnerability Summaries

```bash
cd VulnerabilitySummaryModule/Inference
python generate_summary.py --input ./retrieved_examples.json --output ./summaries.json
```

---

## ðŸ“Š Evaluation Metric

- **Secure Rate (SR)**: Proportion of LLM-generated code that is free from high/critical vulnerabilities, as detected by static analyzers (e.g., Semgrep).
- **Poison Impact (PI)**: Difference in SR between clean and poisoned settings.

---

## ðŸ“Œ Notes

- CodeLlama requires local inference or API wrapper.
- Gemini requires access to Gemini API (via Google Cloud or third-party wrapper).
- All experiments can be extended to integrate with secure prompting strategies.

---

## ðŸ“œ License

MIT License.

---

## âœï¸ Citation

> Coming soon â€“ under submission to [Venue Hidden].

---
